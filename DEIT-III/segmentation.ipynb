{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/new_fact/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from models_v2 import *\n",
    "from src.plots import *\n",
    "#from src.plots_paper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "#from dinov2.models.vision_transformer import vit_large\n",
    "#import dinov2.eval.segmentation.utils.colormaps as colormaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "\n",
    "#image_size = 518\n",
    "image_size = 224\n",
    "patch_size = 14\n",
    "num_classes = 150\n",
    "\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# vit_ckpt = \"/home/azywot/DINOv2/dinov2_vitl14_pretrain.pth\"\n",
    "# save_model_path = \"/home/azywot/DINOv2/saved_models/seg_vitl14_no_registers.pth\"\n",
    "# output_seg_path = \"/home/azywot/DINOv2/saved_models/ADE_val_00001112_segmented.png\"\n",
    "# custom_image_path = \"/home/azywot/DINOv2/ADE_val_00001112.jpg\"\n",
    "# custom_mask_path = \"/home/azywot/DINOv2/ADE_val_00001112_seg.png\"\n",
    "# data_root = \"/home/azywot/DINOv2/ADE20K_2021_17_01/images/ADE\"\n",
    "\n",
    "data_root = \"./images/ADE20K_2021_17_01/images/ADE\"\n",
    "save_model_path = \"./first_try.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ADE20KSegmentation(Dataset):\n",
    "    def __init__(self, root, split=\"training\", image_size=518):\n",
    "        self.image_files = sorted(glob.glob(os.path.join(root, split, \"**\", \"*.jpg\"), recursive=True))\n",
    "        self.image_size = image_size\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        mask_path = img_path.replace(\".jpg\", \"_seg.png\")\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        image = self.transform(image)\n",
    "        mask = np.array(mask)\n",
    "        mask = Image.fromarray(mask).resize((self.image_size, self.image_size), resample=Image.NEAREST)\n",
    "        mask = np.array(mask).astype(np.int64)\n",
    "\n",
    "        mask[(mask != 255) & (mask > 149)] = 149\n",
    "        mask = torch.from_numpy(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "class LinearHead(nn.Module):\n",
    "    def __init__(self, in_dim=1024, num_classes=150, patch_size=14, image_size=518):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.image_size = image_size\n",
    "        self.output_size = (image_size, image_size)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_dim, in_dim, kernel_size=2, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(in_dim, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, patch_tokens):\n",
    "        B, N, C = patch_tokens.shape\n",
    "        H = W = int(N ** 0.5)\n",
    "        patch_tokens = patch_tokens.permute(0, 2, 1).reshape(B, C, H, W)\n",
    "        logits = self.decoder(patch_tokens)\n",
    "        logits = nn.functional.interpolate(logits, size=self.output_size, mode=\"bilinear\", align_corners=False)\n",
    "        return logits\n",
    "\n",
    "class DeitSegModel(nn.Module):\n",
    "    def __init__(self, vit, head, n_reg=0):\n",
    "        super().__init__()\n",
    "        self.vit = vit\n",
    "        self.head = head\n",
    "        self.n_reg = n_reg\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     features = self.vit.forward_features(x)\n",
    "    #     return self.head(features[\"x_norm_patchtokens\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.vit.forward_features(x)\n",
    "        features = self.vit.block_output['final']\n",
    "        if self.n_reg > 0:\n",
    "            return self.head(features[:, 1 : - self.n_reg])\n",
    "        else:\n",
    "            return self.head(features[:, 1 :])\n",
    "    \n",
    "def compute_miou(preds, labels, num_classes, ignore_index=255):\n",
    "    mask = labels != ignore_index\n",
    "    preds = preds[mask]\n",
    "    labels = labels[mask]\n",
    "    cm = confusion_matrix(labels.flatten(), preds.flatten(), labels=list(range(num_classes)))\n",
    "    intersection = np.diag(cm)\n",
    "    union = np.sum(cm, axis=1) + np.sum(cm, axis=0) - np.diag(cm)\n",
    "    iou = intersection / np.maximum(union, 1)\n",
    "    return np.mean(iou), iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ViT-L/14...\n",
      "******************** PRETRAINED 21k MODEL WILL BE USED\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading ViT-L/14...\")\n",
    "#vit = vit_large(patch_size=patch_size, img_size=image_size, init_values=1.0, block_chunks=0, num_register_tokens=0)\n",
    "\n",
    "model_size = \"small\"\n",
    "# we always used 224x224 images for deit\n",
    "image_size = 224\n",
    "side = 14\n",
    "n_reg = 0\n",
    "\n",
    "# Load the pretrained model\n",
    "if model_size == \"tiny\":\n",
    "    vit = deit_tiny_patch16_LS(\n",
    "        pretrained=True, img_size=image_size, pretrained_21k=True\n",
    "    )\n",
    "elif model_size == \"small\":\n",
    "    vit = deit_small_patch16_LS(\n",
    "        pretrained=True, img_size=image_size, pretrained_21k=True\n",
    "    )\n",
    "elif model_size == \"base\":\n",
    "    vit = deit_base_patch16_LS(\n",
    "        pretrained=True, img_size=image_size, pretrained_21k=True\n",
    "    )\n",
    "elif model_size == \"large\":\n",
    "    vit = deit_large_patch16_LS(\n",
    "        pretrained=True, img_size=image_size, pretrained_21k=True\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Invalid model size: choose from 'tiny', 'small', 'base', 'large'\")\n",
    "\n",
    "#vit.load_state_dict(torch.load(vit_ckpt, map_location=\"cpu\"), strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing segmentation head...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Initializing segmentation head...\")\n",
    "# deit patch size is 16\n",
    "head = LinearHead(in_dim = vit.embed_dim, patch_size=16, image_size=image_size)\n",
    "model = DeitSegModel(vit, head, n_reg = n_reg).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Freeze all but last 2 blocks + head\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "# for block in model.vit.blocks[-2:]:\n",
    "#     for p in block.parameters():\n",
    "#         p.requires_grad = True\n",
    "for p in model.head.parameters():\n",
    "    p.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ADE20KSegmentation(data_root, split=\"training\", image_size=image_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   6%|▌         | 92/1599 [02:00<41:50,  1.67s/it]  "
     ]
    }
   ],
   "source": [
    "print(\"Fine-tuning...\")\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "    total_loss = 0\n",
    "    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/5\"):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        preds = model(images)\n",
    "        loss = criterion(preds, masks)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), save_model_path)\n",
    "print(f\"Model saved to {save_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./first_try.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cd/sr6rtt2j1rqdn0jdyblt3ztm0000gn/T/ipykernel_77530/3251492157.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "# load model from save_model_path\n",
    "\n",
    "model = DeitSegModel(vit, head, n_reg = n_reg).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(save_model_path, map_location=device))\n",
    "model.eval()\n",
    "print(\"Model loaded from\", save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on validation set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cd/sr6rtt2j1rqdn0jdyblt3ztm0000gn/T/ipykernel_77530/2979821496.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_model_path))\n",
      "Evaluating: 100%|██████████| 125/125 [01:14<00:00,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation mIoU: 0.0235\n",
      "Evaluating custom test image...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'custom_image_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# ---- Evaluation on custom test image ----\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluating custom test image...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m image = Image.open(\u001b[43mcustom_image_path\u001b[49m).convert(\u001b[33m\"\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m mask = Image.open(custom_mask_path).convert(\u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m transform = transforms.Compose([\n\u001b[32m     32\u001b[39m     transforms.Resize((image_size, image_size)),\n\u001b[32m     33\u001b[39m     transforms.ToTensor(),\n\u001b[32m     34\u001b[39m ])\n",
      "\u001b[31mNameError\u001b[39m: name 'custom_image_path' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- Evaluation on full validation set ----\n",
    "print(\"Evaluating on validation set...\")\n",
    "val_dataset = ADE20KSegmentation(data_root, split=\"validation\", image_size=image_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "model.load_state_dict(torch.load(save_model_path))\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, masks in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        images = images.to(device)\n",
    "        logits = model(images)\n",
    "        preds = logits.argmax(1).cpu()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(masks)\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_labels = torch.cat(all_labels).numpy()\n",
    "\n",
    "miou, iou_per_class = compute_miou(all_preds, all_labels, num_classes=num_classes)\n",
    "print(f\"Validation mIoU: {miou:.4f}\")\n",
    "\n",
    "# ---- Evaluation on custom test image ----\n",
    "# print(\"Evaluating custom test image...\")\n",
    "# image = Image.open(custom_image_path).convert(\"RGB\")\n",
    "# mask = Image.open(custom_mask_path).convert(\"L\")\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((image_size, image_size)),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "# image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "# mask_np = np.array(mask.resize((image_size, image_size), resample=Image.NEAREST)).astype(np.int64)\n",
    "# mask_np[(mask_np != 255) & (mask_np > 149)] = 149\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     pred = model(image_tensor).argmax(1).squeeze(0).cpu().numpy()\n",
    "\n",
    "# colormap = np.array(colormaps.ADE20K_COLORMAP, dtype=np.uint8)\n",
    "# colored_pred = colormap[pred + 1]\n",
    "# Image.fromarray(colored_pred).save(output_seg_path)\n",
    "\n",
    "# miou_test, _ = compute_miou(pred, mask_np, num_classes=num_classes)\n",
    "# print(f\"Test image mIoU: {miou_test:.4f}\")\n",
    "# print(f\"Segmentation saved at {output_seg_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tune + head: mIoU = 0.0235\n",
    "\n",
    "Only head: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_fact",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
